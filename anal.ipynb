{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load('word2vec_model.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('computers', 0.6619974374771118),\n",
       " ('software', 0.5994142889976501),\n",
       " ('digital', 0.47143417596817017),\n",
       " ('minicomputers', 0.44772008061408997),\n",
       " ('machines', 0.4456964433193207),\n",
       " ('supercomputer', 0.4447154998779297),\n",
       " ('toy', 0.444561630487442),\n",
       " ('ibm', 0.43565645813941956),\n",
       " ('equipment', 0.4339454472064972),\n",
       " ('electronic', 0.4326600134372711)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('computer', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = model.wv\n",
    "\n",
    "# saving word vectors in KeyVector format\n",
    "word_vectors.save('word_vectors.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np_word_vectors = model.wv.vectors\n",
    "np.save('np_word_vectors.npy', np_word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'gensim.models.keyedvectors.KeyedVectors'>\n"
     ]
    }
   ],
   "source": [
    "computer_vector = word_vectors.get_vector('computer')\n",
    "supercomputer_vector = word_vectors.get_vector('supercomputer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.44471553\n"
     ]
    }
   ],
   "source": [
    "cos_sim = np.dot(computer_vector, supercomputer_vector) / (np.linalg.norm(computer_vector) * np.linalg.norm(supercomputer_vector))\n",
    "print(cos_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using tsne, in high dimensional cos similarity becomes problematic. perhaps $\\text{dim} = 3$ would be better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 7393 samples in 0.001s...\n",
      "[t-SNE] Computed neighbors for 7393 samples in 0.293s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 7393\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 7393\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 7393\n",
      "[t-SNE] Computed conditional probabilities for sample 4000 / 7393\n",
      "[t-SNE] Computed conditional probabilities for sample 5000 / 7393\n",
      "[t-SNE] Computed conditional probabilities for sample 6000 / 7393\n",
      "[t-SNE] Computed conditional probabilities for sample 7000 / 7393\n",
      "[t-SNE] Computed conditional probabilities for sample 7393 / 7393\n",
      "[t-SNE] Mean sigma: 7.943408\n",
      "[t-SNE] Computed conditional probabilities in 0.102s\n",
      "[t-SNE] Iteration 50: error = 138.7195740, gradient norm = 0.1113852 (50 iterations in 4.987s)\n",
      "[t-SNE] Iteration 100: error = 169.5654755, gradient norm = 0.0215915 (50 iterations in 3.936s)\n",
      "[t-SNE] Iteration 150: error = 169.5790863, gradient norm = 0.0794567 (50 iterations in 5.081s)\n",
      "[t-SNE] Iteration 200: error = 172.6365662, gradient norm = 0.0205027 (50 iterations in 5.441s)\n",
      "[t-SNE] Iteration 250: error = 175.0137177, gradient norm = 0.0084834 (50 iterations in 5.060s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 175.013718\n",
      "[t-SNE] Iteration 300: error = 10.0660753, gradient norm = 0.0125573 (50 iterations in 4.626s)\n",
      "[t-SNE] Iteration 350: error = 9.2830153, gradient norm = 0.0105829 (50 iterations in 4.448s)\n",
      "[t-SNE] Iteration 400: error = 8.7625418, gradient norm = 0.0095544 (50 iterations in 4.570s)\n",
      "[t-SNE] Iteration 450: error = 8.5203323, gradient norm = 0.0089769 (50 iterations in 4.765s)\n",
      "[t-SNE] Iteration 500: error = 8.3215771, gradient norm = 0.0083913 (50 iterations in 5.046s)\n",
      "[t-SNE] Iteration 550: error = 8.1770744, gradient norm = 0.0079914 (50 iterations in 4.962s)\n",
      "[t-SNE] Iteration 600: error = 8.0532246, gradient norm = 0.0076157 (50 iterations in 4.933s)\n",
      "[t-SNE] Iteration 650: error = 7.9405279, gradient norm = 0.0073834 (50 iterations in 5.477s)\n",
      "[t-SNE] Iteration 700: error = 7.8365841, gradient norm = 0.0070329 (50 iterations in 6.343s)\n",
      "[t-SNE] Iteration 750: error = 7.7487297, gradient norm = 0.0067275 (50 iterations in 4.957s)\n",
      "[t-SNE] Iteration 800: error = 7.6637888, gradient norm = 0.0065605 (50 iterations in 5.399s)\n",
      "[t-SNE] Iteration 850: error = 7.5861931, gradient norm = 0.0063698 (50 iterations in 6.829s)\n",
      "[t-SNE] Iteration 900: error = 7.5195761, gradient norm = 0.0060594 (50 iterations in 8.860s)\n",
      "[t-SNE] Iteration 950: error = 7.4658055, gradient norm = 0.0058960 (50 iterations in 9.928s)\n",
      "[t-SNE] Iteration 1000: error = 7.4145055, gradient norm = 0.0057492 (50 iterations in 9.600s)\n",
      "[t-SNE] KL divergence after 1000 iterations: 7.414505\n"
     ]
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "word_vectors_reduced = TSNE(n_components=3,\n",
    "                            verbose = 2).fit_transform(np_word_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7393, 3)\n"
     ]
    }
   ],
   "source": [
    "print(word_vectors_reduced.shape)\n",
    "\n",
    "np.save('word_vectors_reduced.npy', word_vectors_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4728\n",
      "222\n"
     ]
    }
   ],
   "source": [
    "supercomputer_idx = word_vectors.get_index('supercomputer')\n",
    "computer_idx = word_vectors.get_index('computer')\n",
    "\n",
    "print(supercomputer_idx)\n",
    "print(computer_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity between supercomputer and computer: 0.9849855899810791\n"
     ]
    }
   ],
   "source": [
    "print(f\"Cosine Similarity between supercomputer and computer: {np.dot(word_vectors_reduced[supercomputer_idx], word_vectors_reduced[computer_idx]) / (np.linalg.norm(word_vectors_reduced[supercomputer_idx]) * np.linalg.norm(word_vectors_reduced[computer_idx]))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as conjectured prior, cosine similarity performs well in lower $\\mathbb{R}^D$ spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity between supercomputer and emotional: -0.7637559175491333\n"
     ]
    }
   ],
   "source": [
    "emotional_idx = word_vectors.get_index('emotional')\n",
    "\n",
    "print(f\"Cosine Similarity between supercomputer and emotional: {np.dot(word_vectors_reduced[supercomputer_idx], word_vectors_reduced[emotional_idx]) / (np.linalg.norm(word_vectors_reduced[supercomputer_idx]) * np.linalg.norm(word_vectors_reduced[emotional_idx]))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "not similar at all!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity between supercomputer and toshiba: 0.999747633934021\n"
     ]
    }
   ],
   "source": [
    "toshiba_idx = word_vectors.get_index('toshiba')\n",
    "\n",
    "print(f\"Cosine Similarity between supercomputer and toshiba: {np.dot(word_vectors_reduced[supercomputer_idx], word_vectors_reduced[toshiba_idx]) / (np.linalg.norm(word_vectors_reduced[supercomputer_idx]) * np.linalg.norm(word_vectors_reduced[toshiba_idx]))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i shall visualize using projector.tensorflow.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "word_vectors_reduced = np.load('word_vectors_reduced.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('word_vectors.tsv', word_vectors_reduced, delimiter='\\t', fmt = '%.16f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(word_vectors.key_to_index.keys())\n",
    "\n",
    "with open('embeds/word_vector_keys.tsv', 'w') as f:\n",
    "    \n",
    "    for word in keys:\n",
    "        \n",
    "        f.write(f\"{word}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
